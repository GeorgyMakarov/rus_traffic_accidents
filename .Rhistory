print(rpartnum.fit$finalModel)
plot(rpart.fit$finalModel, uniform = TRUE,
main = "Classification Tree")
text(rpart.fit$finalModel, use.n = TRUE, all = TRUE, cex = 0.8)
# there was a difference in prediction on linear model when I used numerical
# levels instead of factors
# there was no difference in decision tree
# train random forest
start <- Sys.time()
rf.fit <- train(sev_class ~.,
data = short_list,
method = "rf",
metric = "ROC",
trControl = ctrl_lm)
stop <- Sys.time()
print(rf.fit$finalModel)
lmnum.fit
rpart.fit
rf.fit
# test prediction on linear model
testnum_list <- testing %>% select(dtpvnum, mon, d, h, distrnum,
roadnum, kts, kuch, sev_class)
head(testnum_list)
glm.pred <- predict(lmnum.fit, testnum_list)
lm_cmatrix <- confusionMatrix(glm.pred, testnum_list$sev_class, positive = "serious")
print(lm_cmatrix)
# test prediction on rpart
test_list <- testing %>% select(dtpv, mon, d, h, district, kts, kuch,
road_cond, sev_class)
head(test_list)
rpart.pred <- predict(rpart.fit, test_list)
rpart_cmatrix <- confusionMatrix(rpart.pred, test_list$sev_class, positive = "serious")
print(rpart_cmatrix)
# test prediction on random forest
rf.pred <- predict(rf.fit, test_list)
rf_cmatrix <- confusionMatrix(rf.pred, test_list$sev_class, positive = "serious")
print(rf_cmatrix)
# I choose decision tree to train further and tune the parameters
# basic parameters tuning
print(rpart.fit$finalModel)
print(rpart_cmatrix)
res2
corrplot(res2$r, type = "upper", order = "hclust",
p.mat = res2$P, sig.level = 0.01, insig = "pch")
cor.test(spearman$kts, spearman$kuch , method = "spearman")
# prerequisite packages
library(dplyr)
library(ggplot2)
library(lubridate)
library(Hmisc)
library(corrplot)
library(caret)
# read data
clean_data <- read.csv("clean_data.csv")
clean_data <- clean_data %>% select(-X)
clean_data <- data.frame(clean_data)
clean_data$date <- ymd(clean_data$date)
clean_data$time <- as.POSIXct(clean_data$time)
clean_data <- clean_data %>% mutate(hour = hour(clean_data$time),
casualties = fatal + injury)
clean_data$month <- month(clean_data$date, label = TRUE)
clean_data$day <- wday(clean_data$date, label = TRUE)
clean_data <- clean_data %>% mutate(cas_type = case_when(
fatal > 0 ~ "fatal",
injury > 0 ~ "injury",
TRUE ~ "non-injury"
))
# add severity rate and severity class to dataset
sev_rate <- clean_data %>%
mutate(kts_kuch = kts + kuch,
severity = casualties / kts_kuch) %>%
mutate(year = year(date),
mmonth = month(date),
mday = mday(date),
hhour = hour(time),
mminute = minute(time)) %>%
mutate(timeline = make_datetime(year, mmonth, mday, hhour, mminute))
sev_rate <- sev_rate %>% mutate(sev_class = case_when(
severity > 0.35 ~ "serious",
severity <= 0.35 ~ "moderate"
))
sev_rate$sev_class <- as.factor(sev_rate$sev_class)
sev_rate$cas_type <- as.factor(sev_rate$cas_type)
sev_rate <- sev_rate %>% select(-c(driving_mode, hour, year, mmonth, mday,
hhour, mminute))
rm(clean_data)
# long-list predictors
# manually choose those which we can collect from a phone call
long_list <- sev_rate %>% select(dtpv, day, month, date, time, district,
kts, kuch, road_cond, sev_class)
# transform month, day, hour to numbers
long_list <- long_list %>%
select(-c(day, month)) %>% mutate(dtpvnum = as.numeric(dtpv),
distrnum = as.numeric(district),
roadnum = as.numeric(road_cond)) %>%
mutate(mon = month(date), d = mday(date), h = hour(time))
# create data partitions for training and testing datasets
set.seed(5860)
in_train <- createDataPartition(y = long_list$sev_class, p = 0.6, list = FALSE)
training <- long_list[in_train,]
testing <- long_list[-in_train,]
dim(training)
# make correlation matrix to check if there are correlated variables
cor_data <- training[, c(5, 6, 9, 10, 11, 12, 13, 14)]
res2 <- rcorr(as.matrix(cor_data))
res2
corrplot(res2$r, type = "upper", order = "hclust",
p.mat = res2$P, sig.level = 0.01, insig = "pch")
# there is slight covariation between kts and kuch and slight covariation
# between accident type and kts - that should not be true, as the levels were
# just marked by alphabet order
# check correlation of kts and kuch with Spearman coefficient
spearman <- training %>% select(kts, kuch) %>% group_by(kts) %>%
summarise(kuch = mean(kuch))
cor.test(spearman$kts, spearman$kuch , method = "spearman")
# spearman rho is 0.922 - there is strong correlation between kts and kuch
# make short-list of variables on training dataset
short_list <- training %>% select(dtpv, mon, d, h, district, kts, kuch,
road_cond, sev_class)
# short-list algorithm
# training control for linear methods
ctrl_lm <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
rpart.fit <- train(sev_class ~.,
data = short_list,
method = "rpart",
metric = "ROC",
trControl = ctrl_lm)
library(rattle)
fancyRpartPlot(rpart.fit$finalModel)
print(rpart.fit$finalModel)
print(lm.fit$finalModel)
rpart_diff <- rpart_stop - rpart_start
num_list <- training %>% select(dtpvnum, mon, d, h, distrnum,
roadnum, kts, kuch, sev_class)
lm_diff <- lm_stop - lm_start
lm_start <- Sys.time()
lm.fit <- train(sev_class ~.,
data = short_list,
method = "glm",
metric = "ROC",
trControl = ctrl_lm)
lm_stop <- Sys.time()
lm_diff <- lm_stop - lm_start
lm_diff
# train decision tree
rpart_start <- Sys.time()
rpart.fit <- train(sev_class ~.,
data = short_list,
method = "rpart",
metric = "ROC",
trControl = ctrl_lm)
rpart_stop <- Sys.time()
rpart_diff <- rpart_stop - rpart_start
# train random forest
rf_start <- Sys.time()
rf.fit <- train(sev_class ~.,
data = short_list,
method = "rf",
metric = "ROC",
trControl = ctrl_lm)
rf_stop <- Sys.time()
rf_diff <- rf_stop - rf_start
print(rf.fit$finalModel)
lm.fit
rpart.fit
lm.fit$ROC
summary(lm.fit)
lm.fit
rpart.fit
rf.fit
rpart.fit
print(rpart.fit$finalModel)
lm.fit
lm.fit$modelInfo
lm.fit$results
lm.fit$results[1]
lm.fit$results[2]
rpart.fit$results[2]
rpart.fit$results[2, 1]
rpart.fit$results[2,]
rpart.fit$results[, 2]
rpart.fit$results[1 , 2]
lm.fit$results[2]
lm.fit
rf.fit
rf.fit$results
rf_roc <- rf.fit[2, 2]
rf_roc <- rf.fit[2]
rf_roc <- rf.fit$results[2, 2]
rf_roc
lm.fit$results
lm_sens <- lm.fit$results[4]
lm_sens
lm_roc <- round(lm.fit$results[2], 4)
lm_roc
lm_roc <- round(lm.fit$results[2], 5)
lm_roc
lm_roc <- round(lm.fit$results[2], 4)
lm_roc
rpart_roc <- round(rpart.fit$results[1, 2], 4)
rf_roc <- round(rf.fit$results[2, 2], 4)
lm.fit$results
lm_sens <- round(lm.fit$results[4], 4)
lm_sens
rpart.fit$results
rpart_sens <- round(rpart.fit$results[1, 4], 4)
rpart_sens
rf.fit$results
rf_sens <- round(rf.fit$results[2, 4], 4)
rf_sens
compare_models <- tribble(~model, ~roc, ~sens, ~train_time,
"glm", lm_roc, lm_sens, lm_diff,
"rpart", rpart_roc, rpart_sens, rpart_diff,
"rf", rf_roc, rf_sens, rf_diff)
compare_models
View(compare_models)
lm_sens
lm_sens[1]
as.numeric(lm_sens)
lm_sens <- as.numeric(lm_sens)
rpart_sens <- as.numeric(rpart_sens)
rf_sens <- as.numeric(rf_sens)
compare_models <- tribble(~model, ~roc, ~sens, ~train_time,
"glm", lm_roc, lm_sens, lm_diff,
"rpart", rpart_roc, rpart_sens, rpart_diff,
"rf", rf_roc, rf_sens, rf_diff)
View(compare_models)
lm_roc <- as.numeric(round(lm.fit$results[2], 4))
rpart_roc <- as.numeric(round(rpart.fit$results[1, 2], 4))
rf_roc <- as.numeric(round(rf.fit$results[2, 2], 4))
compare_models <- tribble(~model, ~roc, ~sens, ~train_time,
"glm", lm_roc, lm_sens, lm_diff,
"rpart", rpart_roc, rpart_sens, rpart_diff,
"rf", rf_roc, rf_sens, rf_diff)
View(compare_models)
compare_models
compare_models
library(rattle)
fancyRpartPlot(rpart.fit$finalModel)
fancyRpartPlot(rf.fit$finalModel)
install.packages("ROSE")
library(ROSE)
data(hacide)
str(hacide.train)
table(hacide.train$cls)
prop.table(table(hacide.train$cls))
library(rpart)
trreimb <- rpart(cls ~., data = hacide.train)
treeimb <- rpart(cls ~., data = hacide.train)
rm(trreimb)
pred.treeimb <- predict(treeimb, newdata = hacide.test)
accuracy.meas(hacide.test$cls, pred.treeimb[, 2])
roc.curve(hacide.test$cls, pred.treeimb[,2], plotit = FALSE)
data_balanced_over <- ovun.sample(cls ~., data = hacide.train, method = "over", N = 1960)$data
table(data_balanced_over$cls)
data_balanced_both <- ovun.sample(cls ~., data = hacide.train, method = "both", p = 0.5, N = 1000, seed = 1)$data
table(data_balanced_both$cls)
data_balanced_both <- ovun.sample(cls ~., data = hacide.train, method = "both", p = 0.5, N = 1000, seed = 5)$data
table(data_balanced_both$cls)
data_balanced_both <- ovun.sample(cls ~., data = hacide.train, method = "both", p = 0.5, N = 1000, seed = 1)$data
table(data_balanced_both$cls)
data.rose <- ROSE(cls ~., data = hacide.train, seed = 1)$data
table(data.rose$cls)
tree.rose <- rpart(cls ~., data = data.rose)
pred.tree.rose <- predict(tree.rose, newdata = hacide.test)
roc.curve(hacide.test$cls, pred.tree.rose[,2])
# prerequisite packages
library(dplyr)
library(ggplot2)
library(lubridate)
library(Hmisc)
library(corrplot)
library(caret)
# read data
clean_data <- read.csv("clean_data.csv")
clean_data <- clean_data %>% select(-X)
clean_data <- data.frame(clean_data)
clean_data$date <- ymd(clean_data$date)
clean_data$time <- as.POSIXct(clean_data$time)
clean_data <- clean_data %>% mutate(hour = hour(clean_data$time),
casualties = fatal + injury)
clean_data$month <- month(clean_data$date, label = TRUE)
clean_data$day <- wday(clean_data$date, label = TRUE)
clean_data <- clean_data %>% mutate(cas_type = case_when(
fatal > 0 ~ "fatal",
injury > 0 ~ "injury",
TRUE ~ "non-injury"
))
# add severity rate and severity class to dataset
sev_rate <- clean_data %>%
mutate(kts_kuch = kts + kuch,
severity = casualties / kts_kuch) %>%
mutate(year = year(date),
mmonth = month(date),
mday = mday(date),
hhour = hour(time),
mminute = minute(time)) %>%
mutate(timeline = make_datetime(year, mmonth, mday, hhour, mminute))
sev_rate <- sev_rate %>% mutate(sev_class = case_when(
severity > 0.35 ~ "serious",
severity <= 0.35 ~ "moderate"
))
sev_rate$sev_class <- as.factor(sev_rate$sev_class)
sev_rate$cas_type <- as.factor(sev_rate$cas_type)
sev_rate <- sev_rate %>% select(-c(driving_mode, hour, year, mmonth, mday,
hhour, mminute))
rm(clean_data)
# long-list predictors
# manually choose those which we can collect from a phone call
long_list <- sev_rate %>% select(dtpv, day, month, date, time, district,
kts, kuch, road_cond, sev_class)
# transform month, day, hour to numbers
long_list <- long_list %>%
select(-c(day, month)) %>% mutate(dtpvnum = as.numeric(dtpv),
distrnum = as.numeric(district),
roadnum = as.numeric(road_cond)) %>%
mutate(mon = month(date), d = mday(date), h = hour(time))
# create data partitions for training and testing datasets
set.seed(5860)
in_train <- createDataPartition(y = long_list$sev_class, p = 0.6, list = FALSE)
training <- long_list[in_train,]
testing <- long_list[-in_train,]
dim(training)
# make correlation matrix to check if there are correlated variables
cor_data <- training[, c(5, 6, 9, 10, 11, 12, 13, 14)]
res2 <- rcorr(as.matrix(cor_data))
res2
corrplot(res2$r, type = "upper", order = "hclust",
p.mat = res2$P, sig.level = 0.01, insig = "pch")
# there is slight covariation between kts and kuch and slight covariation
# between accident type and kts - that should not be true, as the levels were
# just marked by alphabet order
# check correlation of kts and kuch with Spearman coefficient
spearman <- training %>% select(kts, kuch) %>% group_by(kts) %>%
summarise(kuch = mean(kuch))
cor.test(spearman$kts, spearman$kuch , method = "spearman")
# spearman rho is 0.922 - there is strong correlation between kts and kuch
# make short-list of variables on training dataset
short_list <- training %>% select(dtpv, mon, d, h, district, kts, kuch,
road_cond, sev_class)
# short-list algorithm
# training control for linear methods
ctrl_lm <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
summaryFunction = twoClassSummary,
classProbs = TRUE)
# train lm
num_list <- training %>% select(dtpvnum, mon, d, h, distrnum,
roadnum, kts, kuch, sev_class)
lm_start <- Sys.time()
lm.fit <- train(sev_class ~.,
data = short_list,
method = "glm",
metric = "ROC",
trControl = ctrl_lm)
lm_stop <- Sys.time()
lm_diff <- lm_stop - lm_start
# train decision tree
rpart_start <- Sys.time()
rpart.fit <- train(sev_class ~.,
data = short_list,
method = "rpart",
metric = "ROC",
trControl = ctrl_lm)
rpart_stop <- Sys.time()
rpart_diff <- rpart_stop - rpart_start
# train random forest
rf_start <- Sys.time()
rf.fit <- train(sev_class ~.,
data = short_list,
method = "rf",
metric = "ROC",
trControl = ctrl_lm)
rf_stop <- Sys.time()
rf_diff <- rf_stop - rf_start
#make comparison table for models
lm_roc <- as.numeric(round(lm.fit$results[2], 4))
rpart_roc <- as.numeric(round(rpart.fit$results[1, 2], 4))
rf_roc <- as.numeric(round(rf.fit$results[2, 2], 4))
lm_sens <- as.numeric(round(lm.fit$results[4], 4))
rpart_sens <- as.numeric(round(rpart.fit$results[1, 4], 4))
rf_sens <- as.numeric(round(rf.fit$results[2, 4], 4))
compare_models <- tribble(~model, ~roc, ~sens, ~train_time,
"glm", lm_roc, lm_sens, lm_diff,
"rpart", rpart_roc, rpart_sens, rpart_diff,
"rf", rf_roc, rf_sens, rf_diff)
compare_models
#plot decision tree
library(rattle)
fancyRpartPlot(rpart.fit$finalModel)
# test prediction on linear model
testnum_list <- testing %>% select(dtpvnum, mon, d, h, distrnum,
roadnum, kts, kuch, sev_class)
head(testnum_list)
glm.pred <- predict(lmnum.fit, testnum_list)
lm_cmatrix <- confusionMatrix(glm.pred, testnum_list$sev_class, positive = "serious")
print(lm_cmatrix)
# test prediction on rpart
test_list <- testing %>% select(dtpv, mon, d, h, district, kts, kuch,
road_cond, sev_class)
head(test_list)
rpart.pred <- predict(rpart.fit, test_list)
rpart_cmatrix <- confusionMatrix(rpart.pred, test_list$sev_class, positive = "serious")
print(rpart_cmatrix)
# test prediction on random forest
rf.pred <- predict(rf.fit, test_list)
rf_cmatrix <- confusionMatrix(rf.pred, test_list$sev_class, positive = "serious")
print(rf_cmatrix)
# I choose decision tree to train further and tune the parameters
# basic parameters tuning
print(rpart.fit$finalModel)
print(rpart_cmatrix)
rf.fit
library(ROSE)
rf.pred <- predict(rf.fit, test_list)
rf_cmatrix <- confusionMatrix(rf.pred, test_list$sev_class, positive = "serious")
print(rf_cmatrix)
rf.pred
print(rf_cmatrix)
roc.curve(test_list$sev_class, rf.pred)
head(short_list)
rf.fit$finalModel
rf.pred.train <- pred(rf.fit, short_list)
rf.pred.train <- predict(rf.fit, short_list)
roc.curve(short_list$sev_class, rf.pred.train)
rf.fit
table(short_list$sev_class)
prop.table(table(short_list$sev_class))
rf.pred <- predict(rf.fit, test_list)
rf_cmatrix <- confusionMatrix(rf.pred, test_list$sev_class, positive = "serious")
print(rf_cmatrix)
prop.table(table(short_list$sev_class))
test_list <- testing %>% select(dtpv, mon, d, h, district, kts, kuch,
road_cond, sev_class)
rf.pred <- predict(rf.fit, test_list)
rf_cmatrix <- confusionMatrix(rf.pred, test_list$sev_class,
positive = "serious")
print(rf_cmatrix)
rf.pred.imb <- predict(rf.fit, test_list)
rf_cmatrix <- confusionMatrix(rf.pred.imb, test_list$sev_class,
positive = "serious")
print(rf_cmatrix)
roc.curve(test_list$sev_class, rf.pred.imb, plotit = FALSE)
prop.table(table(short_list$sev_class))
table(data.rose$cls)
short_list_balanced <- ROSE(sev_class ~., data = short_list, seed = 1)$data
table(short_list_balanced$sev_class)
short_list_balanced <- ROSE(sev_class ~., data = short_list, seed = 1, N = 7000)$data
table(short_list_balanced$sev_class)
short_list_balanced <- ROSE(sev_class ~., data = short_list, seed = 1)$data
table(short_list_balanced$sev_class)
rfbal_start <- Sys.time()
rf.fit.bal <- train(sev_class ~.,
data = short_list_balanced,
method = "rf",
metric = "ROC",
trControl = ctrl_lm)
rfbal_stop <- Sys.time()
rfbal_diff <- rfbal_stop - rfbal_start
rfbal_diff
rf_diff
rf.fit.bal
print(rf.fit.bal$finalModel)
print(rf.fit$finalModel)
rf.pred.bal <- predict(rf.fit.bal, test_list)
rfbal_matrix <- confusionMatrix(rf.pred.bal, test_list$sev_class,
positive = "serious")
print(rfbal_matrix)
roc.curve(test_list$sev_class, rf.pred.bal, plotit = FALSE)
short_balanced_2 <- ROSE(sev_class ~., data = short_list, seed = 1, N = 7000)$data
table(short_balanced_2$sev_class)
rfbal_start <- Sys.time()
rf.fit.bal2 <- train(sev_class ~.,
data = short_list_balanced,
method = "rf",
metric = "ROC",
trControl = ctrl_lm)
rfbal_stop <- Sys.time()
rfbal_diff <- rfbal_stop - rfbal_start
rfbal_diff
rf.fit.bal2
print(rf.fit.bal2$finalModel)
print(rf.fit$finalModel)
rf.pred.bal2 <- predict(rf.fit.bal2, test_list)
rfbal2_matrix <- confusionMatrix(rf.pred.bal2, test_list$sev_class,
positive = "serious")
print(rfbal2_matrix)
print(rfbal_matrix)
rpart.fit
rpart.pred <- predict(rpart.fit, test_list)
rpart_m <- confusionMatrix(rpart.pred, test_list$sev_class,
positive = "serious")
print(rpart_m)
roc.curve(test_list$sev_class, rf.pred.bal2, plotit = FALSE)
roc.curve(test_list$sev_class, rpart.pred, plotit = FALSE)
rpart.fit.bal <- train(sev_class ~.,
data = short_list_balanced,
method = "rpart",
metric = "ROC",
trControl = ctrl_lm)
rpart.pred2 <- predict(rpart.fit.bal, test_list)
rpart_m2 <- confusionMatrix(rpart.pred.bal, test_list$sev_class,
positive = "serious")
rpart_m2 <- confusionMatrix(rpart.pred2, test_list$sev_class,
positive = "serious")
print(rpart_m2)
roc.curve(test_list$sev_class, rpart.pred2, plotit = FALSE)
